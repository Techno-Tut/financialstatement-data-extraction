{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3710e63f",
   "metadata": {},
   "source": [
    "# Credit Card Statement Analysis Pipeline\n",
    "\n",
    "This notebook creates a pipeline that:\n",
    "1. Uses LlamaParse to extract text from PDF statements\n",
    "2. Processes the text with LangChain and GPT-4\n",
    "3. Structures the data using Pydantic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b94121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate virtual environment (adjust path as needed)\n",
    "import os\n",
    "os.system('source ../venv/bin/activate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!source ../venv/bin/activate && pip install langchain langchain_openai langchain_community pydantic pypdf llama-parse python-dotenv requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339451a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "from datetime import date\n",
    "import json\n",
    "\n",
    "# For LlamaParse\n",
    "from llama_cloud_services import LlamaParse\n",
    "\n",
    "# For LangChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# For data models\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# if not os.environ.get('OPENAI_API_KEY'):\n",
    "#     os.environ['OPENAI_API_KEY'] = \"sk-...\"  # Replace with your OpenAI API key\n",
    "\n",
    "# # LlamaParse API key (uncomment if not in .env file)\n",
    "# os.environ['LLAMA_CLOUD_API_KEY'] = \"llama-...\"  # Replace with your LlamaParse API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70591c4f",
   "metadata": {},
   "source": [
    "## Define Pydantic Models for Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditCardTransaction(BaseModel):\n",
    "    transaction_date: date = Field(\n",
    "        description=\"Date when the transaction occurred (YYYY-MM-DD)\"\n",
    "    )\n",
    "    posting_date: Optional[date] = Field(\n",
    "        default=None,\n",
    "        description=\"Date when the transaction was posted (YYYY-MM-DD). May be missing in some statements.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Text description of the transaction, including merchant name or payment info\"\n",
    "    )\n",
    "    amount: float = Field(\n",
    "        description=\"Transaction amount in dollars. Negative for charges, positive for credits/refunds\"\n",
    "    )\n",
    "\n",
    "class CreditCardStatement(BaseModel):\n",
    "    \"\"\"\n",
    "    A model representing structured information extracted from a credit card statement, \n",
    "    including customer info, billing period, account metadata, and itemized transactions.\n",
    "    \"\"\"\n",
    "\n",
    "    first_name: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"First name of the credit card holder\"\n",
    "    )\n",
    "    last_name: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Last name of the credit card holder\"\n",
    "    )\n",
    "    account_number: str = Field(\n",
    "        description=\"Last 4 or 5 digits of the credit card number, extracted exactly as shown at the end of the account reference (e.g., '91003'). Do not include any masked or partial prefixes like 'XXXX XXXX'.\"\n",
    "    )\n",
    "    card_type: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Type of credit card (e.g. Visa, Amex, Mastercard)\"\n",
    "    )\n",
    "    statement_period: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Month and year of the statement, e.g., 'February 2024'\"\n",
    "    )\n",
    "    statement_opening_date: Optional[date] = Field(\n",
    "        default=None,\n",
    "        description=\"Opening date of the billing cycle in YYYY-MM-DD format\"\n",
    "    )\n",
    "    statement_closing_date: Optional[date] = Field(\n",
    "        default=None,\n",
    "        description=\"Closing date of the billing cycle in YYYY-MM-DD format\"\n",
    "    )\n",
    "    credit_limit: Optional[float] = Field(\n",
    "        default=None,\n",
    "        description=\"Credit limit of the card in dollars\"\n",
    "    )\n",
    "    transactions: List[CreditCardTransaction] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of all transactions that occurred during the billing cycle\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cab962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionType(Enum):\n",
    "    DEPOSIT = \"deposit\"\n",
    "    WITHDRAWN = \"withdrawn\"\n",
    "\n",
    "class ChequingTransaction(BaseModel):\n",
    "    transaction_date: date = Field(\n",
    "        description=\"Date when the transaction occurred (YYYY-MM-DD)\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Text description of the transaction, including merchant name or payment info\"\n",
    "    )\n",
    "    transaction_type: TransactionType = Field(\n",
    "        description=\"Type of transaction: 'deposit' or 'withdrawn'\"\n",
    "    )\n",
    "    amount: float = Field(\n",
    "        description=\"The amount of the transaction, either debit or credit\"\n",
    "    )\n",
    "    ledger_balance: float = Field(\n",
    "        description=\"Account balance after the transaction in dollars\"\n",
    "    )\n",
    "\n",
    "class ChequingAccountStatement(BaseModel):\n",
    "    \"\"\"\n",
    "    A model representing structured information extracted from a chequing account statement,\n",
    "    including customer info, statement period, account metadata, and itemized transactions.\n",
    "    \"\"\"\n",
    "    opening_balance: float = Field(\n",
    "        description=\"Opening balance of the account for the statement period\"\n",
    "    )\n",
    "    closing_balance: float = Field(\n",
    "        description=\"Closing balance of the account for the statement period\"\n",
    "    )\n",
    "    bank_name: str = Field(\n",
    "        description=\"Name of the bank or financial institution\"\n",
    "    )\n",
    "    account_number: str = Field(\n",
    "        description=\"Last 4 or 5 digits of the account number, extracted exactly as shown at the end of the account reference (e.g., '91003'). Do not include any masked or partial prefixes like 'XXXX XXXX'.\"\n",
    "    )\n",
    "    statement_opening_date: Optional[date] = Field(\n",
    "        default=None,\n",
    "        description=\"Opening date from the statement in YYYY-MM-DD format for which the statement is generated and the opening balance is provided\"\n",
    "    )\n",
    "    statement_closing_date: Optional[date] = Field(\n",
    "        default=None,\n",
    "        description=\"Closing date from the statement in YYYY-MM-DD format for which the statement is generated and the closing balance is provided\"\n",
    "    )\n",
    "    transactions: List[ChequingTransaction] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of all transactions that occurred during the statement period\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69d25b",
   "metadata": {},
   "source": [
    "## Step 1: PDF Text Extraction with LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFExtractor:\n",
    "    def __init__(self, preset=\"complexTables\"):\n",
    "        self.api_key = os.environ.get('LLAMA_CLOUD_API_KEY')\n",
    "        self.preset = preset\n",
    "        \n",
    "        try:\n",
    "            # Initialize LlamaParse with API key\n",
    "            if self.api_key:\n",
    "                self.parser = LlamaParse(api_key=self.api_key, preset=self.preset)\n",
    "                self.use_llama = True\n",
    "                print(f\"Using LlamaParse for PDF extraction with '{self.preset}' preset\")\n",
    "            else:\n",
    "                raise ValueError(\"LLAMA_CLOUD_API_KEY not found in environment variables\")\n",
    "        except Exception as e:\n",
    "            # Fallback to PyPDF if LlamaParse is not available\n",
    "            self.use_llama = False\n",
    "            print(f\"LlamaParse not available: {e}\\nFalling back to PyPDF\")\n",
    "    \n",
    "    def extract_text(self, pdf_path):\n",
    "        \"\"\"Extract text from PDF using LlamaParse or fallback to PyPDF\"\"\"\n",
    "        try:\n",
    "            if self.use_llama:\n",
    "                # Create status tracker\n",
    "                parse_status = {\"status\": \"processing\", \"file\": pdf_path}\n",
    "                \n",
    "                # Start the parsing job with the specified preset\n",
    "                print(f\"Starting LlamaParse job with {self.preset} preset...\")\n",
    "                \n",
    "                # Submit document for parsing\n",
    "                document = self.parser.parse(\n",
    "                    file_path=pdf_path\n",
    "                )\n",
    "                \n",
    "                # Check if parsing was successful and extract text\n",
    "                if hasattr(document, 'text'):\n",
    "                    extracted_text = document.text\n",
    "                elif hasattr(document, 'content'):\n",
    "                    extracted_text = document.content\n",
    "                else:\n",
    "                    # Access raw document content as fallback\n",
    "                    extracted_text = str(document)\n",
    "                \n",
    "                # Update status\n",
    "                parse_status[\"status\"] = \"complete\"\n",
    "                parse_status[\"text_length\"] = len(extracted_text)\n",
    "                parse_status[\"job_id\"] = getattr(document, 'job_id', 'unknown')\n",
    "                \n",
    "                return {\n",
    "                    \"status\": parse_status,\n",
    "                    \"text\": extracted_text\n",
    "                }\n",
    "            else:\n",
    "                # Fallback to PyPDF\n",
    "                reader = PdfReader(pdf_path)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                \n",
    "                return {\n",
    "                    \"status\": {\"status\": \"complete\", \"file\": pdf_path, \"text_length\": len(text)},\n",
    "                    \"text\": text\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text: {e}\")\n",
    "            return {\n",
    "                \"status\": {\"status\": \"error\", \"file\": pdf_path, \"error\": str(e)},\n",
    "                \"text\": \"\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fc196",
   "metadata": {},
   "source": [
    "## Step 2: Define LangChain Pipeline for Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29189dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class AbstractProcessor(ABC):\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-4.1-nano\")\n",
    "        self.chain = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_prompt(self):\n",
    "        \"\"\"Set the prompt template for the processor\"\"\"\n",
    "        pass\n",
    "\n",
    "    def process(self, statement_text: str):\n",
    "        try:\n",
    "            # Log the raw text for debugging\n",
    "            print(f\"🔍 Raw statement text (first 500 chars): {statement_text[:500]}\")\n",
    "\n",
    "            # Run the chain\n",
    "            result = self.chain.invoke({\n",
    "                \"statement_text\": statement_text,\n",
    "                \"format_instructions\": self.parser.get_format_instructions()\n",
    "            })\n",
    "\n",
    "            # Parse the output\n",
    "            parsed_data = self.parser.parse(result[\"structured_data\"])\n",
    "            return parsed_data\n",
    "\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Validation error: {ve}\")\n",
    "        except Exception as e:\n",
    "            print(f\"LLM parsing error: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "class CreditCardProcessor(AbstractProcessor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.parser = PydanticOutputParser(pydantic_object=CreditCardStatement)\n",
    "        self.set_prompt()\n",
    "\n",
    "    def set_prompt(self):\n",
    "        prompt_template = \"\"\"\n",
    "        You are a financial document analyst specialized in credit card statements.\n",
    "\n",
    "        Your task is to extract structured data from a credit card statement.\n",
    "\n",
    "        Return the data in a JSON object matching this format:\n",
    "        {format_instructions}\n",
    "\n",
    "        Extract ONLY the fields that are explicitly mentioned in the text. \n",
    "        If something is missing or unclear, return null or leave the field blank.\n",
    "        Never guess or invent information.\n",
    "\n",
    "        Statement text:\n",
    "        {statement_text}\n",
    "        \"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        self.chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.prompt,\n",
    "            output_key=\"structured_data\"\n",
    "        )\n",
    "\n",
    "class BankAccountProcessor(AbstractProcessor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ChequingAccountStatement)\n",
    "        self.set_prompt()\n",
    "\n",
    "    def set_prompt(self):\n",
    "        prompt_template = \"\"\"\n",
    "        You are a financial document analyst specialized in bank account statements.\n",
    "\n",
    "        Your task is to extract structured data from a chequing account statement.\n",
    "\n",
    "        Return the data in a JSON object matching this format:\n",
    "        {format_instructions}\n",
    "\n",
    "        ⚠️ Important instructions:\n",
    "        - Only include real monetary transactions such as deposits, withdrawals, transfers, bill payments, etc.\n",
    "        - Do NOT include the \"Opening Balance\" or \"Closing Balance\" lines as transactions. These should only be used to populate the `opening_balance` and `closing_balance` fields.\n",
    "        - Exclude summaries or headers from the `transactions` list.\n",
    "        - If a transaction spans multiple lines, merge them into a single description.\n",
    "\n",
    "        Statement text:\n",
    "        {statement_text}\n",
    "        \"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        self.chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.prompt,\n",
    "            output_key=\"structured_data\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47488f",
   "metadata": {},
   "source": [
    "## Step 3: Complete Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ce91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class StatementType(Enum):\n",
    "    CREDIT_CARD = \"credit_card\"\n",
    "    CHEQUING = \"chequing\"\n",
    "\n",
    "class FinancialStatementPipeline:\n",
    "    def __init__(self):\n",
    "        self.extractor = PDFExtractor()\n",
    "    \n",
    "    def get_processor(self, statement_type: StatementType):\n",
    "        \"\"\"Factory method to get the appropriate processor based on statement type\"\"\"\n",
    "        if statement_type == StatementType.CREDIT_CARD:\n",
    "            return CreditCardProcessor()\n",
    "        elif statement_type == StatementType.CHEQUING:\n",
    "            return BankAccountProcessor()\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported statement type: {statement_type}')\n",
    "    \n",
    "    def process_statement(self, pdf_path, statement_type: StatementType):\n",
    "        \"\"\"Process a single financial statement\"\"\"\n",
    "        extraction_result = self.extractor.extract_text(pdf_path)\n",
    "        status = extraction_result[\"status\"]\n",
    "        text = extraction_result[\"text\"]\n",
    "\n",
    "        print(f\"📝 Extraction status: {status['status']}\")\n",
    "\n",
    "        if status[\"status\"] != \"complete\":\n",
    "            print(f\"❌ Error extracting text from {pdf_path}: {status.get('error', 'Unknown error')}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"🔍 Processing statement text ({status.get('text_length', 0)} characters)...\")\n",
    "        processor = self.get_processor(statement_type)\n",
    "        structured_data = processor.process(text)\n",
    "\n",
    "        return {\n",
    "            \"status\": status,\n",
    "            \"structured_data\": structured_data\n",
    "        }\n",
    "\n",
    "    def run(self, pdf_path: str, statement_type: StatementType):\n",
    "        print(f\"📄 Processing: {pdf_path}\")\n",
    "        result = self.process_statement(pdf_path, statement_type)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb9b32",
   "metadata": {},
   "source": [
    "## Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = FinancialStatementPipeline()\n",
    "results = pipeline.run(\"./data/Statements.pdf\", StatementType.CHEQUING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date\n",
    "\n",
    "if results and \"structured_data\" in results:\n",
    "    structured_data_json = results[\"structured_data\"].model_dump()\n",
    "\n",
    "    # Convert date and enum objects to strings for JSON serialization\n",
    "    def custom_converter(obj):\n",
    "        if isinstance(obj, date):\n",
    "            return obj.isoformat()\n",
    "        if isinstance(obj, Enum):\n",
    "            return obj.value\n",
    "        raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "    with open(\"./results/structured_data.json\", \"w\") as json_file:\n",
    "        json.dump(structured_data_json, json_file, indent=2, default=custom_converter)\n",
    "    print(\"Structured data has been written to ./results/structured_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7402fd",
   "metadata": {},
   "source": [
    "## Pipeline Visualization\n",
    "\n",
    "The following is a text representation of the pipeline flow:\n",
    "\n",
    "```\n",
    "PDF Files (Scotia.pdf, Amex.pdf)\n",
    "    ↓\n",
    "PDFExtractor (using LlamaParse)\n",
    "    ↓\n",
    "Raw Text + Extraction Status\n",
    "    ↓\n",
    "LangChain Processor\n",
    "    ↓ [ChatPromptTemplate → ChatOpenAI (GPT-4) → PydanticOutputParser]\n",
    "Structured Data (CreditCardStatement)\n",
    "    ↓\n",
    "Markdown Report + JSON Data\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
